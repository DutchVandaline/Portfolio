<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Project 2</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="icon" href="../images/telescope.png" type="image/png">
    <link rel="stylesheet" href="project.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>

</head>
<body>
    <div class="top-menu">
        <a href="../index.html">Junha</a>
        <a href="../ai_development.html">AI</a>
        <a href="../app_development.html">App</a>
        <a href="../contact.html">Contact</a>
        <i id="darkModeToggle" class="fas fa-moon"></i>
    </div>
    <div class="container">
        <header>
            <h1 class="title">Pretrain BERT from Scratch</h1>
            <div class="author-info">
                <span class="author">By Junha Park</span>
                <span class="date">Published on Jan 27, 2025</span>
            </div>
        </header>
        <main>
            <div class="image-container">
                <img src="../blog_images/Bert.jpg" alt="sealant">
            </div>
            <section class="paragraph">
                <h2>Overview</h2>
                <p>
                    Let me get it straight. Yes, I've builT BERT model from scratch and yes, I've pretrained it from scratch. The first thing that popped up in my head was that pretraining BERT might not be that difficult. Pretrain is similar to fine-tuning so, how hard can it be?
                    Also, all the guidlines were on paper, even organized into a blog post on Kaggle, Hugging Face, Medium or so. I thought it will take approximately 3 days for gathering data, preprocessing it, and start training.
                    I admit that I was naive. BERT is made by Google, the gigantic tech company that utilizes tons of GPU and TPUs. Also, BERT isn't made by one person. It was a team. Following is my path of pretraining BERT.
                </p>
            </section>
            <section class="paragraph">
                <h2>Data Preprocess</h2>
                <p>
                    I thought everything was going well... until I realized that 3B is just a number of parameters and it doens't mean the size of datasets. 
                    I already knew that numbers that were used to measure the size of model like, "8B" or "175B" was the size of parameters, legitimately. 
                    The thing that I didn't know was the size of dataset is about 100 ~ 1000 times bigger that it's parameter size.
                    My first goal was to make 3B tokens datasets. BERT used about 330B tokens but, it may take too much time training, I've decided to use 1% of it. <br>
                    Dataset should contain NSP and MLM which stands for "Next Sentence Prediction" and "Masked Language Models" each. I've gathered 'BookCorpus' and 'Wikipedia' which was used at BERT original paper. 
                    'BookCorpus' dataset was a set of novel lines, but 'Wikipedia' dataset was a set of chunks of articles. 
                    Which means, that I need to split articles into each lines. Everything worked fine, straight-forward. So far, so good.
                </p>
                <h3> Making Dataset</h3>
                <p>
                    To fulfill my goal, 3B datasets, I used Hugging face package. Downloading Wikipedia costs too much time and memory. After fetching from Hugging Face package, started splitting it into lines. 
                    Used `Bert-Tokenizer` to tokenize and make a dataset. Problem occurred here.
                    It took way too long for handling the Dataset. I've ran for 15 hours making positive and negative sets for NSP task, tokenizing it, adding attention mask, turning into `.pt` file but, it won't finish.
                    Plus, GPU was running for the other task, heating up over 85℃ and killed my computer. It kept repeating over and over. <br>
                    I've decided to kill the process and handle step by step. First, no GPU task while preprocessing the data. Second, make positive, negative set and label into jsonl file. Third, tokenize and make NSP, MLM dataset into `.pt` file.
                    Of course, it took way too long so I've modified my goal from 3B to 1B tokens. It seems simple, but this preprocess took about 4 days. There were a lot of things happened back then.
                <pre><code class="language-json">
[
    {
        "sentence1": "usually , he would be tearing around the living room , playing with his toys .",
        "sentence2": "but just one look at a minion sent him practically catatonic .",
        "label": 1
    },
    {
        "sentence1": "usually , he would be tearing around the living room , playing with his toys .",
        "sentence2": "if she asked him to leave what were the chances he would go ?",
        "label": 0
    },
    ... ...
]
                </code></pre>
                </p>
            </section>
            <section class="paragraph">
                <h2>Result + Hurdles</h2>
                <p>
                    
                </p>
            </section>
        </main>
    </div>
    <script>
        // 다크 모드 토글
        const darkModeToggle = document.getElementById('darkModeToggle');
        let darkModeEnabled = false;

        if (localStorage.getItem('darkMode') === 'enabled') {
            darkModeEnabled = true;
            document.body.classList.add('dark-mode');
            darkModeToggle.classList.remove('fa-moon');
            darkModeToggle.classList.add('fa-lightbulb');
        }

        darkModeToggle.addEventListener('click', () => {
            darkModeEnabled = !darkModeEnabled;
            document.body.classList.toggle('dark-mode', darkModeEnabled);

            if (darkModeEnabled) {
                darkModeToggle.classList.remove('fa-moon');
                darkModeToggle.classList.add('fa-lightbulb');
                localStorage.setItem('darkMode', 'enabled');
            } else {
                darkModeToggle.classList.remove('fa-lightbulb');
                darkModeToggle.classList.add('fa-moon');
                localStorage.setItem('darkMode', 'disabled');
            }
        });

        const navLinks = document.querySelectorAll('.top-menu a');

        navLinks.forEach(link => {
            link.addEventListener('click', (event) => {
                event.preventDefault(); 
                const targetUrl = link.getAttribute('href');
                location.replace(targetUrl);
            });
        });

    </script>
    
</body>
</html>
